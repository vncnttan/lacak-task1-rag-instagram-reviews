{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook used to preprocess the data and generate the embeddings file\n",
    "This notebook is used to preprocess the data and generate the embeddings file using all-mpnet-base-v2 model. The embeddings file consists of the embeddings of the reviews. This file will be fetched on RAG when the user queries the system with a specific prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_description</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The app is good for connecting with friends, f...</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07-11 23:57:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Used to be my favorite social media app, but \"...</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-22 21:37:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instagram is the best of all the social media....</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-07-25 03:24:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love this app.. but as of late, I have been ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2023-07-09 04:49:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Used to be a great app but there are so many m...</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07-17 16:47:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  review_description  rating  \\\n",
       "0  The app is good for connecting with friends, f...       3   \n",
       "1  Used to be my favorite social media app, but \"...       2   \n",
       "2  Instagram is the best of all the social media....       5   \n",
       "3  I love this app.. but as of late, I have been ...       2   \n",
       "4  Used to be a great app but there are so many m...       3   \n",
       "\n",
       "           review_date  \n",
       "0  2023-07-11 23:57:07  \n",
       "1  2023-07-22 21:37:09  \n",
       "2  2023-07-25 03:24:58  \n",
       "3  2023-07-09 04:49:57  \n",
       "4  2023-07-17 16:47:04  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Instagram Review Data\n",
    "df = pd.read_csv(\"./instagram-play-store-reviews/instagram.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word_count': 90, 'review_token_count': 124.0, 'review_char_count': 496, 'rating': 3, 'review': \"The app is good for connecting with friends, family and even potential business partners. However as of recently I've experienced some problems with the messages portion of the app (ex: themes aren't showing up on my end but are present on other person's end). Idk if it has to do with a bug but it happened all of sudden out of nowhere on both of my pages (one private the other public). But besides the occasional bugs and sometimes the app/website being down randomly, I say it's a decent app.\", 'review_date': '2023-07-11 23:57:07'}\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Reviews and Ratings\n",
    "def preprocess_reviews(df: pd.DataFrame) -> list:\n",
    "    rating_and_reviews = []\n",
    "    for index, row in df.iterrows():\n",
    "        # Handle Missing Values\n",
    "        if not row[\"review_description\"] or not row[\"rating\"]:\n",
    "            continue\n",
    "\n",
    "        # Handle Empty Reviews\n",
    "        if row[\"review_description\"] == \"\" or row[\"review_description\"].isspace():\n",
    "            continue\n",
    "        \n",
    "        rating_and_reviews.append({\n",
    "            \"word_count\": len(row[\"review_description\"].split(\" \")),\n",
    "            \"review_token_count\": len(row[\"review_description\"]) / 4,\n",
    "            \"review_char_count\": len(row[\"review_description\"]),\n",
    "            \"rating\": row[\"rating\"],\n",
    "            \"review\": row[\"review_description\"],\n",
    "            \"review_date\": row[\"review_date\"]\n",
    "        })\n",
    "\n",
    "    return rating_and_reviews\n",
    "\n",
    "rating_and_reviews = preprocess_reviews(df)\n",
    "print(rating_and_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rating_and_reviews)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further text processing (splitting pages into sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210542/210542 [00:41<00:00, 5129.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for item in tqdm(rating_and_reviews):\n",
    "    doc = nlp(item[\"review\"])\n",
    "    item[\"sentences\"] = list(doc.sents)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    item[\"sentence_count_spacy\"] = len(list(doc.sents))\n",
    "\n",
    "print(type(rating_and_reviews[0][\"sentences\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rating_and_reviews)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking sentences\n",
    "# This is done to split the reviews into smaller chunks of sentences -> effective for reviews that are too long. This will be split into chunks of 10 sentences each.\n",
    "num_sentence_chunk_size = 10\n",
    "\n",
    "def chunk_sentences(sentences: list, chunk_size: int) -> list:\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), chunk_size):\n",
    "        chunks.append(sentences[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "\n",
    "for item in tqdm(rating_and_reviews):\n",
    "    item[\"sentence_chunks\"] = chunk_sentences(item[\"sentences\"], num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rating_and_reviews)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting each chunk into its own item\n",
    "import re\n",
    "\n",
    "rating_and_review_chunks = []\n",
    "for item in tqdm(rating_and_reviews):\n",
    "    for chunk in item[\"sentence_chunks\"]:\n",
    "        joined_sentence_chunk = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r\"\\.([A-Z])\", r\". \\1\", joined_sentence_chunk)\n",
    "\n",
    "        rating_and_review_chunks.append({\n",
    "            \"rating\": item[\"rating\"],\n",
    "            \"review\": item[\"review\"],\n",
    "            \"review_date\": item[\"review_date\"],\n",
    "            \"sentence_chunk\": joined_sentence_chunk,\n",
    "            \"chunk_char_count\": len(joined_sentence_chunk),\n",
    "            \"chunk_word_count\": len(joined_sentence_chunk.split(\" \")),\n",
    "            \"chunk_token_count\": len(joined_sentence_chunk) / 4\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rating_and_review_chunks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create more meaningful chunks, we can filter out chunks that are too short\n",
    "# This reviews are spammy and do not provide any meaningful \n",
    "# The minimum token length is set to 20 -> ~5 words per chunk\n",
    "min_token_length = 20\n",
    "for row in df[df[\"chunk_token_count\"] < min_token_length].sample(10).iterrows():\n",
    "    print(f\"Chunk token count: {row[1]['sentence_chunk']} | Text: {row[1]['sentence_chunk']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_and_rating_chunks_over_min_token_length = df[df[\"chunk_token_count\"] >= min_token_length].to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all-mpnet-base-v2 Sentence Transformer Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embedding_model.to(\"cuda\")\n",
    "text_chunks = [item[\"sentence_chunk\"] for item in review_and_rating_chunks_over_min_token_length]\n",
    "batch_size = 32\n",
    "\n",
    "# Embedding each chunk of text using the Sentence Transformer Model and storing the embeddings\n",
    "for i in tqdm(range(0, len(text_chunks), batch_size)):\n",
    "    batch = text_chunks[i:i + batch_size]\n",
    "    embeddings = embedding_model.encode(batch)\n",
    "\n",
    "    for j, e in enumerate(embeddings):\n",
    "        review_and_rating_chunks_over_min_token_length[i + j][\"chunk_embedding\"] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to a file\n",
    "text_chunks_embeddings_df = pd.DataFrame(review_and_rating_chunks_over_min_token_length)\n",
    "text_chunks_embeddings_df.to_csv(\"embeddings/text_chunks_embeddings-20token.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_ai_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
